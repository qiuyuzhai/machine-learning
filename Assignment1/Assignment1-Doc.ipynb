{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r5J15zrOh-O"
   },
   "source": [
    "**Name:** zhai qiuyu\n",
    "\n",
    "**EID:** 5999 1830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oPpaCuVOh-P"
   },
   "source": [
    "# CS5489 - Assignment 1 - SMS classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBrgd11BOh-P"
   },
   "source": [
    "## Goal\n",
    "In this assignment, the task is predict whether an SMS message is a real message, a spam message, or a phishing message (called smishing). Here are some examples:\n",
    "\n",
    "  - **Normal**: \"For real tho this sucks. I can't even cook my whole electricity is out. And I'm hungry.\"\n",
    "  - **Spam**: \"Had your mobile 10 mths? Update to latest Orange camera/video phones for FREE. Save £s with Free texts/weekend calls. Text YES for a callback orno to opt out\"\n",
    "  - **Smishing**: \"Todays Vodafone numbers ending 5347 are selected to receive a Rs.2,00,000 award. If you have a match please call 6299257179 quoting claim code 2041 standard rates apply\"\n",
    "\n",
    "\n",
    "Your goal is to train a classifier to predict the class from the SMS text.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "You need to train classifiers using the training data, and then predict on the test data. You are free to choose the feature extraction method and classifier algorithm.  You are free to use methods that were not introduced in class.  You should probably do cross-validation to select a good parameters.\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "You need to report your test predictions. The csv file has determined the split of validation and test data, where the validation data will be used to determine the timestep of checkpointing. The model that achieves the best performance on validation set should be used to evaluate the test data. The test performance will be used to calculate your final ranking.\n",
    "\n",
    "The evaluation metric is **balanced accuracy score**. This is because the dataset has some class imbalance as there are more normal samples than spam/smishing samples. See details for `sklearn.metrics.balanced_accuracy_score` [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html).\n",
    "\n",
    "## What to hand in\n",
    "You need to turn in the following things:\n",
    "\n",
    "1. This ipynb file `Assignment1-Doc.ipynb` with your source code and documentation. _**You should write about all the various attempts that you make to find a good solution.**_ You may also submit python scripts as source code, but you then must document all analysis and results (figures, outputs, etc.) in the ipynb file.\n",
    "2. The PDF file exported from your `Assignment1-Doc.ipynb` file.\n",
    "3. Your final CSV submission file on test data.\n",
    "4. The ipynb file `Assignment1-Final.ipynb`, which contains the code that generates the final CSV submission file.  **This code will be used to verify that your submission is reproducible.**\n",
    "\n",
    "**Please compress all four files into a single zipfile and upload it to Assignment 1 on Canvas.**\n",
    "\n",
    "## Basic Requirements of Documentation:\n",
    "\n",
    "For your documentation, you need to at least explain the following things:\n",
    "\n",
    "- **Data Preprocessing**: This section should detail your exploratory data analysis and the rationale behind all preprocessing steps.\n",
    "  - Describe the initial characteristics of your dataset.\n",
    "  - Explain all techniques you have applied on the data\n",
    "  - Clearly state which subset of the data was used to determine any inherent hyperparameters within your preprocessing techniques, ensuring no information from the test set was used (if any)\n",
    "- **Methodology**: you will justify your modeling choices.\n",
    "  - Chosen Models: describe the core principle or model architecture (for deep learning)\n",
    "  - Chosen Optimizers (if any)\n",
    "  - Chosen Loss Function (if any)\n",
    "- **Hyperparameters**: This section should demonstrate a rigorous approach to hyper-parameter selection\n",
    "  - List the key hyperparameters used\n",
    "  - Document your hyperparameter search process. Compare model performance on a dedicated validation set and select the best-performing configuration based solely on validation metrics. As a core principle of this course, using the test set for hyperparameter selection is strictly prohibited and constitutes academic dishonesty.\n",
    "- **Results and Visualization**: This section should provide clear evidence of your model's performance and a qualitative analysis of its behavior.\n",
    "  - Learning Curves (only for deep learning methods): the loss (error) and/or accuracy on training and validation set must be provided.\n",
    "  - Show examples of correctly classified and misclassified test samples. For misclassified samples, hypothesize why the model failed.\n",
    "\n",
    "## Grading\n",
    "The marks of the assignment are distributed as follows:\n",
    "- 45% - Results using various classifiers and feature representations.\n",
    "- 30% - Trying out feature representations (e.g. adding additional features) or classifiers not used in the tutorials/lectures.\n",
    "- 20% - Quality of the written report.  More points for insightful observations and analysis.\n",
    "- 5% - Final performance on the test data. If a submission cannot be reproduced by the submitted code, it will not receive marks for ranking.\n",
    "- **Late Penalty:** 25 marks will be subtracted for each day late.\n",
    "\n",
    "**NOTE:** This is an _individual_ assignment.\n",
    "\n",
    "**NOTE:** you should start early! Some classifiers may take a while to train.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRhPFj_-Oh-Q"
   },
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVxkPYcfOh-R"
   },
   "source": [
    "The training data is in the text file `smishing_train.txt`.  This CSV file contains the SMS text and the class label. The class labels are: `0`, `1`, `2`, which are `normal`, `spam`, `smishing`.\n",
    "\n",
    "The validation/testing data is in the text file `smishing_test.txt`, and only contains the SMS text.\n",
    "\n",
    "The label of validation/testing data is in the csv file `smishing_val_test.csv`, and only contains the SMS text.\n",
    "\n",
    "You need to generate a csv file, with the following format:\n",
    "\n",
    "<pre>\n",
    "Id,Prediction\n",
    "1,0\n",
    "2,1\n",
    "3,0\n",
    "4,2\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpP69t_AOh-R"
   },
   "source": [
    "Here are two helpful functions for reading the text data and writing the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:41:59.292781Z",
     "start_time": "2023-09-23T11:41:57.895983Z"
    },
    "id": "c57EEbVrOh-R"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:00.281663Z",
     "start_time": "2023-09-23T11:42:00.260443Z"
    },
    "id": "O54nN2MMOh-R"
   },
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "\n",
    "    txtdata = []            # 文本数据\n",
    "    classes = []            # 标签\n",
    "    with open(fname, 'r', encoding='utf-8') as csvfile:  # open file safely with UTF-8\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            txtdata.append(row[0])# row[0]是文本数据\n",
    "            if len(row)>1:# 如果有标签\n",
    "                classes.append(int(row[1]))# row[1]是标签\n",
    "\n",
    "    return (txtdata, classes) # 这样就变成了一个元组（文本数据，标签）\n",
    "\n",
    "def write_csv(fname, Y):\n",
    "    tmp = [['Id', 'Prediction']] # CSV 标题\n",
    "\n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "\n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wnXAmRoOh-S"
   },
   "source": [
    "The below code will load the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:01.278832Z",
     "start_time": "2023-09-23T11:42:01.245768Z"
    },
    "id": "u-o33XRoOh-S",
    "outputId": "37653590-4b3b-4912-b070-4e6eb1684682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985\n",
      "2986\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Prediction Usage\n",
       "0   1           0   val\n",
       "1   2           0  test\n",
       "2   3           0   val\n",
       "3   4           0  test\n",
       "4   5           0   val"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "(traintxt, trainY) = read_text_data(\"smishing_train.txt\")\n",
    "(testtxt, _)       = read_text_data(\"smishing_val_test.txt\")\n",
    "testY              = pd.read_csv(\"smishing_val_test.csv\")\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))\n",
    "testY.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dunno da next show aft 6 is 850. Toa payoh got 650.',\n",
       " 'I.ll hand her my phone to chat wit u',\n",
       " 'I dont have i shall buy one dear',\n",
       " 'Nite...',\n",
       " 'Ok�congrats�']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintxt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Prediction Usage\n",
       "0   1           0   val\n",
       "1   2           0  test\n",
       "2   3           0   val\n",
       "3   4           0  test\n",
       "4   5           0   val"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:01.907735Z",
     "start_time": "2023-09-23T11:42:01.895439Z"
    },
    "id": "pZc9C4jLOh-S",
    "outputId": "bf8eec35-0d6d-4ff2-d906-67c842321f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# 展示类别\n",
    "classnames = unique(trainY)\n",
    "print(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:03.483048Z",
     "start_time": "2023-09-23T11:42:03.477589Z"
    },
    "id": "qbBNgWpaOh-S"
   },
   "outputs": [],
   "source": [
    "classlabels = ['normal', 'spam', 'smishing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDNpQ5f6Oh-S"
   },
   "source": [
    "Here is an example to write a csv file with predictions on the test set.  These are random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:04.088144Z",
     "start_time": "2023-09-23T11:42:04.058459Z"
    },
    "id": "Cf_2YjFoOh-T"
   },
   "outputs": [],
   "source": [
    "# random.randint(n, size=m)，从0到n-1中随机取m个\n",
    "i = random.randint(len(classnames), size=len(testtxt))\n",
    "predY = classnames[i]\n",
    "write_csv(\"my_submission.csv\", predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhVZ11-YOh-T"
   },
   "source": [
    "Look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T11:42:04.621862Z",
     "start_time": "2023-09-23T11:42:04.608007Z"
    },
    "id": "ZMQdzdsiOh-T",
    "outputId": "f78b4a6a-2146-435e-9d41-9549c8845546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[normal]: Dunno da next show aft 6 is 850. Toa payoh got 650.\n",
      "[normal]: I.ll hand her my phone to chat wit u\n",
      "[normal]: I dont have i shall buy one dear\n",
      "[normal]: Nite...\n",
      "[normal]: Ok�congrats�\n",
      "[spam]: I'd like to tell you my deepest darkest fantasies. Call me 09094646631 just 60p/min. To stop texts call 08712460324 (nat rate)\n",
      "[spam]: Santa Calling! Would your little ones like a call from Santa Xmas eve? Call 09058094583 to book your time.\n",
      "[spam]: Meet Top 35 US universities in Delhi at India Habitat Centre Lodhi Road on Nov 8th, 2 to 6 pm for student admission.Entry Free,  details contact 9911489000\n",
      "[spam]: SMS AUCTION You have won a Nokia 7250i. This is what you get when you win our FREE auction. To take part send Nokia to 86021 now. HG/Suite342/2Lands Row/W1JHL 16+\n",
      "[spam]: Call Germany for only 1 pence per minute! Call from a fixed line via access number 0844 861 85 86.\n",
      "[smishing]: WIN URGENT! Your mobile number has been awarded with a £2000 prize GUARANTEED call 09061790121 from land line. claim 3030 valid 12hrs only 150ppm\n",
      "[smishing]: Customer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n",
      "[smishing]: Todays Vodafone numbers ending 9167 are selected to receive a Rs.2,00,000 award. If you have a match please call 7044518857 quoting claim code 5001 standard rates apply\n",
      "[smishing]: \tFree 1st week entry 2 TEXTPOD 4 a chance 2 win 40GB iPod or £250 cash every wk. Txt VPOD to 81303 Ts&Cs www.textpod.net custcare 08712405020.\n",
      "[smishing]: 449050000301 You have won a £2,000 price! To claim, call 09050000301.\n"
     ]
    }
   ],
   "source": [
    "for c in classnames:# 每个类别\n",
    "    tmp = where(trainY==c)\n",
    "    for a in tmp[0][0:5]:# 当前类别的前5个\n",
    "        print('[{}]: {}'.format(classlabels[trainY[a]], traintxt[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务清单\n",
    "- 数据预处理：\n",
    "1.数据集的初始特征（如样本数量、特征类型、类别分布、缺失值情况、异常值特征等）\n",
    "2.解释你在数据上应用的所有技术（如缺失值填充、异常值处理、特征编码、特征归一化 / 标准化、特征选择等）\n",
    "3.明确说明你使用了数据集中的哪一个子集（如训练集、验证集）来确定预处理技术中固有的超参数（例如，用训练集的均值 / 标准差进行标准化、用训练集的分布确定异常值阈值等），并确保未使用测试集中的任何信息（若存在测试集）\n",
    "\n",
    "- 模型选择：\n",
    "1.所选模型的核心原理或架构（仅针对深度学习）\n",
    "2.优化器\n",
    "3.损失函数\n",
    "\n",
    "- 超参数：\n",
    "1.列出使用的关键超参数\n",
    "2.记录超参数的搜索过程：在专门的验证集上对比模型性能，并完全基于验证集的评估指标来选择表现最佳的参数配置。\n",
    "3.作为本课程的核心原则，严禁使用测试集进行超参数选择，这种行为属于学术不端。\n",
    "\n",
    "- 结果与可视化：\n",
    "1.学习曲线（仅针对深度学习方法）：需提供训练集和验证集上的损失值（误差）和 / 或准确率。\n",
    "2.展示测试集中分类正确与分类错误的样本示例：对于分类错误的样本，需推测模型分类失败的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据排查\n",
    "- 缺失值：对 `traintxt/testtxt` 统计空字符串与仅空白样本，看是否需要占位符等操作。\n",
    "- 异常值：用长度（字符数）、数字字符比率、URL/号码命中等统计量做探查，识别极端长短信、号码/URL 极多的样本。\n",
    "- 填充与处理：\n",
    "  - 缺失文本或特殊字符以占位符 \"<EMPTY>\" 替换，以便模型进行学习；\n",
    "  - 对于有的过长的短信，可以直接截断，防止信息冗杂；\n",
    "- 特征缩放：\n",
    "  - 文本特征可以采用 `TfidfVectorizer`''，包含 L2 归一化；\n",
    "  - 对于数值派生特征（长度、数字占比等），可以做 `StandardScaler`等处理，并严格以训练集统计量拟合（避免信息泄漏）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train desc: {'count': 2985, 'empty_count': 0, 'len_mean': 82.17219430485763, 'len_p95': 161.0, 'len_max': 910, 'digit_ratio_mean': 0.02400560015137386, 'url_ratio': 0.03149078726968174, 'phone_ratio': 0.10452261306532663, 'amt_ratio': 0.05862646566164154}\n",
      "Test desc  : {'count': 2986, 'empty_count': 0, 'len_mean': 84.30709979906229, 'len_p95': 161.0, 'len_max': 611, 'digit_ratio_mean': 0.02580820242487473, 'url_ratio': 0.030475552578700604, 'phone_ratio': 0.1188881446751507, 'amt_ratio': 0.06965840589417281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\2720230533.py:13: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n",
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\2720230533.py:31: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  'empty_count': int(sum(is_empty_text(t) for t in texts)),\n"
     ]
    }
   ],
   "source": [
    "# 数据诊断：缺失值与异常值探查\n",
    "import re\n",
    "\n",
    "# 若是空字符串或仅包含空白字符，则返回 True\n",
    "def is_empty_text(t: str) -> bool:\n",
    "    return (t is None) or (len(t.strip()) == 0)\n",
    "\n",
    "def describe_texts(texts):\n",
    "    # 计算每条文本的字符长度（包含空格等）\n",
    "    lengths = np.array([len(t) for t in texts])\n",
    "    # 计算每条文本中数字字符占比：数字字符数 / 最大(1, 文本长度)（避免除以0）\n",
    "    digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n",
    "    # URL 的正则（匹配 http:// https:// 或以 www. 开头的域名）\n",
    "    url_pat = re.compile(r\"https?://|www\\.\")\n",
    "    # 匹配至少7位连续数字的正则（常用于检测长电话号码）\n",
    "    phone_pat = re.compile(r\"\\b\\d{7,}\\b\")\n",
    "    # 金额符号的正则（匹配 £ $ € ¥ 或 rs/inr/usd/gbp 等货币缩写，忽略大小写）\n",
    "    amt_pat = re.compile(r'[£$€¥]|\\b(?:rs|inr|usd|gbp)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # 对每条文本检测是否包含 URL（1 表示命中，0 表示未命中）\n",
    "    url_hits = np.array([1 if url_pat.search(t.lower()) else 0 for t in texts])\n",
    "    # 对每条文本检测是否包含长号码（1/0）\n",
    "    phone_hits = np.array([1 if phone_pat.search(t) else 0 for t in texts])\n",
    "    # 对每条文本检测是否包含金额符号（1/0）\n",
    "    amt_hits = np.array([1 if amt_pat.search(t) else 0 for t in texts])\n",
    "\n",
    "    return {\n",
    "        'count': len(texts),\n",
    "        'empty_count': int(sum(is_empty_text(t) for t in texts)),\n",
    "        'len_mean': float(lengths.mean()),\n",
    "        'len_p95': float(np.percentile(lengths, 95)),\n",
    "        'len_max': int(lengths.max()),\n",
    "        'digit_ratio_mean': float(digit_ratio.mean()),\n",
    "        'url_ratio': float(url_hits.mean()),\n",
    "        'phone_ratio': float(phone_hits.mean()),\n",
    "        'amt_ratio': float(amt_hits.mean()),\n",
    "    }\n",
    "\n",
    "train_desc = describe_texts(traintxt)\n",
    "test_desc   = describe_texts(testtxt)\n",
    "print('Train desc:', train_desc)\n",
    "print('Test desc  :', test_desc)\n",
    "\n",
    "# 为后续向量化准备占位符，避免空文本报错\n",
    "traintxt_proc = [t if not is_empty_text(t) else '<EMPTY>' for t in traintxt]\n",
    "test_proc   = [t if not is_empty_text(t) else '<EMPTY>' for t in testtxt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据输出可以知道：\n",
    "- 文本数都为0，说明没有完全空的条目，非常好。\n",
    "- 训练集和验证集在总体分布上很接近（len_mean,len_p95,digit_ratio_mean,url_ratio），说明分割合理，非常好。\n",
    "- 训练集有极端值到910,训练集有极端值到611，说明存在少数超长短信。这超长文本可能包含大量冗余信息（如重复话术、无关内容），若直接参与特征提取（如 TF-IDF），会因词数多而放大低频 / 无意义词的权重，稀释关键特征（如欺诈关键词）的影响；并且模型可能对 “长度” 产生不当依赖（比如误认为 “长文本更可能是垃圾短信”），导致过拟合。解决方案：用 TF-IDF 的 L2 归一化和过滤罕见词，本质是消除 “长度” 对特征权重的干扰。进一步，对于处理过后还是很长的SMS，可以直接截断，因为短信信息一般集中在前面，这样可以避免冗余内容误导模型。\n",
    "- URL 和长号码 数量很少，但它们对区分 spam / smishing 可能具有高信息量。解决方案：将它们替换为占位token，比如<URL>,<PHONE>,将 “具体实例” 抽象为 “类别特征”—— 让模型聚焦于 “这类符号的存在本身”，而非具体内容，既防止字典极度稀疏（每个URL/号码都是独立特征），又强化 “特殊符号与垃圾短信的关联”。\n",
    "- digit_ratio 平均很低，但某些样本数字占比较高的情况（如金额、电话号码）可能仍是重要特征。解决方案：可以把货币符号或金额标准化为<AMOUNT>，使特征更通用且不会稀释字典。\n",
    "- 对于这些数字类的，其数值范围差异大，若直接输入模型，会因 “量纲不一致” 导致模型对高数值样本过度敏感（或对低数值样本忽略）。解决方案：对它们进行StandardScaler 或者 RobustScaler 标准化，让 “数字占比的相对高低” 成为模型可识别的信号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- standardscaler:基于数据的 “中心趋势（均值）” 和 “离散程度（标准差）” 进行缩放，让转换后的数据均值为 0，标准差为 1。缺点：对极端值非常敏感。因为均值和标准差会被极端值严重拉高或拉低，导致标准化结果失真。\n",
    "- RobustScaler:基于数据的 “中间位置（中位数）” 和 “中间 50% 数据的离散程度（IQR）” 进行缩放，不受极端值影响。优点：对极端值不敏感（稳健）。因为中位数和 IQR 仅由数据的中间 50% 决定，极端值不会影响这两个统计量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中正常邮件的数量是：2454\n",
      "训练集中垃圾邮件的数量是：234\n",
      "训练集中诈骗邮件的数量是：297\n",
      "训练集总邮件数：2985\n"
     ]
    }
   ],
   "source": [
    "# 统计训练集各类邮件数量（假设 0=正常，1=垃圾，2=广告）\n",
    "normal_count = trainY.count(0)  # 统计标签为0的数量（正常邮件）\n",
    "spam_count = trainY.count(1)    # 统计标签为1的数量（垃圾邮件）\n",
    "ad_count = trainY.count(2)      # 统计标签为2的数量（广告邮件）\n",
    "\n",
    "# 打印结果\n",
    "print(f\"训练集中正常邮件的数量是：{normal_count}\")\n",
    "print(f\"训练集中垃圾邮件的数量是：{spam_count}\")\n",
    "print(f\"训练集中诈骗邮件的数量是：{ad_count}\")\n",
    "print(f\"训练集总邮件数：{len(traintxt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据结果，看来存在类别不平衡的问题，正常邮件偏多，垃圾和诈骗邮件偏少。可以在模型中使用class_weight='balanced'，它会根据样本中各类别的占比，自动调整不同类别的 “损失权重”。权重与类别频率成反比，避免模型因多数类（正常邮件）占比过高而 “偏向” 预测多数类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\3604261003.py:28: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n"
     ]
    }
   ],
   "source": [
    "# 预处理 + 缩放（先仅处理训练集；验证/测试在划分后再处理）\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# 正则：URL、长电话号码、货币/金额符号\n",
    "url_pat = re.compile(r'https?://|www\\.')\n",
    "phone_pat = re.compile(r'\\b\\d{7,}\\b')\n",
    "amt_pat = re.compile(r'[£$€¥]|\\b(?:rs|inr|usd|gbp)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "def normalize_text(t,max_length=200):  # 把具体实例替换为类别特征,截断文本长度\n",
    "    if t is None or len(str(t).strip()) == 0:\n",
    "        return '<EMPTY>'\n",
    "    s = str(t).lower()\n",
    "    s = url_pat.sub(' <URL> ', s)\n",
    "    s = phone_pat.sub(' <PHONE> ', s)\n",
    "    s = amt_pat.sub(' <AMOUNT> ', s)\n",
    "    s = ' '.join(s.split())\n",
    "    if len(s) > max_length:\n",
    "        s = s[:max_length]  # 仅保留前max_length字符\n",
    "    return s\n",
    "\n",
    "traintxt_norm = [normalize_text(t, max_length=200) for t in traintxt]\n",
    "\n",
    "# 提取数值衍生特征：长度、数字占比、是否含 URL、是否含长号码\n",
    "def make_numeric_features(texts):\n",
    "    lengths = np.array([len(t) for t in texts])\n",
    "    digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n",
    "    url_hits = np.array([1 if url_pat.search(t) else 0 for t in texts])\n",
    "    phone_hits = np.array([1 if phone_pat.search(t) else 0 for t in texts])\n",
    "    return np.vstack([lengths, digit_ratio, url_hits, phone_hits]).T  # 行=样本，列=特征\n",
    "\n",
    "train_num = make_numeric_features(traintxt_norm)\n",
    "\n",
    "# 选择缩放器：若长度 99 百分位 > 500，则使用 RobustScaler，否则 StandardScaler（仅在训练集拟合）\n",
    "scaler = RobustScaler() if np.percentile(train_num[:, 0], 99) > 500 else StandardScaler()\n",
    "scaler.fit(train_num)\n",
    "train_num_s = scaler.transform(train_num)\n",
    "\n",
    "# 转换为稀疏格式，便于与 TF-IDF 矩阵 hstack\n",
    "train_num_csr = csr_matrix(train_num_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么选择 TF‑IDF 而不是 BoW\n",
    "\n",
    "- BoW (CountVectorizer) 记录的是词频（每个词在短信中出现的次数），它对短文本任务有用，但存在问题：当某些短信非常长或包含重复模板时，计数会放大这些文本的影响，从而导致训练过程中“长度”或重复模式主导模型。\n",
    "- 而 TF‑IDF 通过 IDF 抑制高频低信息词并结合归一化减少长度偏差，这在包含极端长短信和模版化内容的短文本分类任务中尤为重要。通过同时使用词级与字符级 TF‑IDF，我们既能捕捉有意义的词语/短语，又能对 URL/电话号码等字符模式保持鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985, 56293)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统一的 TF-IDF 向量化（词与字符），仅在训练集 fit；验证/测试在划分后 transform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 词级与字符级 TF-IDF\n",
    "word_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2, max_features=30000)\n",
    "char_vect = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), min_df=2, max_features=60000)\n",
    "\n",
    "# 仅在训练文本上拟合词表/IDF\n",
    "trainX_word = word_vect.fit_transform(traintxt_norm)\n",
    "trainX_char = char_vect.fit_transform(traintxt_norm)\n",
    "\n",
    "# 仅 TF-IDF（非负），供 MultinomialNB 使用\n",
    "trainX_tfidf = hstack([trainX_word, trainX_char]).tocsr()\n",
    "\n",
    "# 拼接训练特征，并叠加数值衍生特征（供其他模型使用）\n",
    "trainX = hstack([trainX_word, trainX_char, train_num_csr]).tocsr()\n",
    "trainX.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先把验证集和测试集单独整理出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集样本数: 1493，测试集样本数: 1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\3604261003.py:28: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n",
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\3604261003.py:28: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  digit_ratio = np.array([sum(ch.isdigit() for ch in t) / max(1, len(t)) for t in texts])\n"
     ]
    }
   ],
   "source": [
    "# 基于提供的 Usage 字段先划分验证与测试索引\n",
    "val_mask = (testY['Usage'].astype(str).str.lower() == 'val').values\n",
    "test_mask = (testY['Usage'].astype(str).str.lower() == 'test').values\n",
    "val_indices = np.where(val_mask)[0]\n",
    "test_indices = np.where(test_mask)[0]\n",
    "\n",
    "# 准备验证/测试标签\n",
    "val_true = testY.loc[val_mask, 'Prediction'].astype(int).values\n",
    "test_true = testY.loc[test_mask, 'Prediction'].astype(int).values\n",
    "\n",
    "# 提取验证/测试文本并做与训练一致的token规范化\n",
    "valtxt = [testtxt[i] for i in val_indices]\n",
    "valtxt_norm = [normalize_text(t, max_length=200) for t in valtxt]\n",
    "\n",
    "testtxt_sel = [testtxt[i] for i in test_indices]\n",
    "testtxt_norm = [normalize_text(t, max_length=200) for t in testtxt_sel]\n",
    "\n",
    "# 生成验证/测试的数值衍生特征，并用训练拟合好的 scaler 变换\n",
    "val_num = make_numeric_features(valtxt_norm)\n",
    "val_num_s = scaler.transform(val_num)\n",
    "val_num_csr = csr_matrix(val_num_s)\n",
    "\n",
    "test_num = make_numeric_features(testtxt_norm)\n",
    "test_num_s = scaler.transform(test_num)\n",
    "test_num_csr = csr_matrix(test_num_s)\n",
    "\n",
    "# 用训练拟合好的 TF-IDF 向量器变换验证/测试文本\n",
    "valX_word = word_vect.transform(valtxt_norm)\n",
    "valX_char = char_vect.transform(valtxt_norm)\n",
    "# 仅 TF-IDF（非负），供 MultinomialNB 使用\n",
    "valX_tfidf = hstack([valX_word, valX_char]).tocsr()\n",
    "# TF-IDF + 数值特征（供其他模型使用）\n",
    "valX = hstack([valX_word, valX_char, val_num_csr]).tocsr()\n",
    "\n",
    "testX_word = word_vect.transform(testtxt_norm)\n",
    "testX_char = char_vect.transform(testtxt_norm)\n",
    "# 仅 TF-IDF（非负），供 MultinomialNB 使用\n",
    "testX_tfidf = hstack([testX_word, testX_char]).tocsr()\n",
    "# TF-IDF + 数值特征（供其他模型使用）\n",
    "testX = hstack([testX_word, testX_char, test_num_csr]).tocsr()\n",
    "\n",
    "# 准备验证集标签\n",
    "val_y = val_true.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉验证与评分工具\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=100)\n",
    "#分层 K 折交叉验证的配置，核心作用是在模型训练时更合理地划分训练集和验证集，尤其适合分类不平衡任务\n",
    "scorer = 'balanced_accuracy'\n",
    "\n",
    "def run_grid_search(estimator, param_grid, X, y, cv=cv, scorer=scorer, n_jobs=-1):\n",
    "    gs = GridSearchCV(estimator, param_grid, cv=cv, scoring=scorer, n_jobs=n_jobs, verbose=1, return_train_score=False)\n",
    "    gs.fit(X, y)\n",
    "    cvlog = pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)#按平均测试分数降序排序\n",
    "    return gs, cvlog\n",
    "\n",
    "# 将验证集标签准备为numpy\n",
    "val_y = val_true.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "MNB best params: {'alpha': 0.5} val balanced_acc: 0.8039547947804828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.795333</td>\n",
       "      <td>0.782661</td>\n",
       "      <td>0.765327</td>\n",
       "      <td>0.781107</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "      <td>0.701243</td>\n",
       "      <td>0.699023</td>\n",
       "      <td>0.696063</td>\n",
       "      <td>0.698776</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032840</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'alpha': 1.5}</td>\n",
       "      <td>0.630666</td>\n",
       "      <td>0.621064</td>\n",
       "      <td>0.586636</td>\n",
       "      <td>0.612788</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.034881      0.002231         0.006504        0.001939         0.5   \n",
       "1       0.036020      0.000000         0.006036        0.000000         1.0   \n",
       "2       0.032840      0.002804         0.007875        0.001301         1.5   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.5}           0.795333           0.782661           0.765327   \n",
       "1  {'alpha': 1.0}           0.701243           0.699023           0.696063   \n",
       "2  {'alpha': 1.5}           0.630666           0.621064           0.586636   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.781107        0.012299                1  \n",
       "1         0.698776        0.002122                2  \n",
       "2         0.612788        0.018904                3  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MultinomialNB 超参数搜索（使用仅 TF-IDF 的非负特征）\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_params = {\n",
    "    'alpha': [0.5, 1.0, 1.5],\n",
    "}\n",
    "# 注意：MultinomialNB 需要非负特征，故使用 trainX_tfidf/valX_tfidf\n",
    "mnb_gs, mnb_log = run_grid_search(MultinomialNB(), mnb_params, trainX_tfidf, trainY)\n",
    "\n",
    "mnb_best = mnb_gs.best_estimator_\n",
    "val_pred = mnb_best.predict(valX_tfidf)\n",
    "mnb_val_bacc = balanced_accuracy_score(val_y, val_pred)\n",
    "print('MNB best params:', mnb_gs.best_params_, 'val balanced_acc:', mnb_val_bacc)\n",
    "\n",
    "mnb_log.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以下是 “通用版本”，支持二分类 / 多分类，所以没特意指明multi_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "LR best params: {'C': 2.0} val balanced_acc: 0.8765334648362172\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546306</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>2.0</td>\n",
       "      <td>{'C': 2.0}</td>\n",
       "      <td>0.836845</td>\n",
       "      <td>0.846706</td>\n",
       "      <td>0.820658</td>\n",
       "      <td>0.834737</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.617505</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'C': 4.0}</td>\n",
       "      <td>0.841934</td>\n",
       "      <td>0.838974</td>\n",
       "      <td>0.816792</td>\n",
       "      <td>0.832567</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460342</td>\n",
       "      <td>0.011093</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'C': 1.0}</td>\n",
       "      <td>0.828797</td>\n",
       "      <td>0.828390</td>\n",
       "      <td>0.824116</td>\n",
       "      <td>0.827101</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.414881</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.822894</td>\n",
       "      <td>0.825023</td>\n",
       "      <td>0.819311</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "2       0.546306      0.038604         0.005243        0.003756     2.0   \n",
       "3       0.617505      0.026777         0.006778        0.001596     4.0   \n",
       "1       0.460342      0.011093         0.006163        0.001700     1.0   \n",
       "0       0.414881      0.013643         0.006054        0.001587     0.5   \n",
       "\n",
       "       params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "2  {'C': 2.0}           0.836845           0.846706           0.820658   \n",
       "3  {'C': 4.0}           0.841934           0.838974           0.816792   \n",
       "1  {'C': 1.0}           0.828797           0.828390           0.824116   \n",
       "0  {'C': 0.5}           0.810016           0.822894           0.825023   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "2         0.834737        0.010738                1  \n",
       "3         0.832567        0.011220                2  \n",
       "1         0.827101        0.002117                3  \n",
       "0         0.819311        0.006630                4  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression 超参数搜索（Linear，类不平衡）\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=None, solver='liblinear')\n",
    "lr_params = {\n",
    "    'C': [0.5, 1.0, 2.0, 4.0],\n",
    "}\n",
    "lr_gs, lr_log = run_grid_search(lr, lr_params, trainX, trainY)\n",
    "\n",
    "lr_best = lr_gs.best_estimator_\n",
    "val_pred = lr_best.predict(valX)\n",
    "lr_val_bacc = balanced_accuracy_score(val_y, val_pred)\n",
    "print('LR best params:', lr_gs.best_params_, 'val balanced_acc:', lr_val_bacc)\n",
    "\n",
    "lr_log.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "LinearSVC best params: {'C': 0.5} val balanced_acc: 0.8887162361933004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14774\\.conda\\envs\\py38tflow2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.843833</td>\n",
       "      <td>0.618862</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>0.842341</td>\n",
       "      <td>0.838974</td>\n",
       "      <td>0.826893</td>\n",
       "      <td>0.836070</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.589835</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'C': 4.0}</td>\n",
       "      <td>0.847521</td>\n",
       "      <td>0.818513</td>\n",
       "      <td>0.822212</td>\n",
       "      <td>0.829416</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.531672</td>\n",
       "      <td>0.106731</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'C': 1.0}</td>\n",
       "      <td>0.843655</td>\n",
       "      <td>0.822787</td>\n",
       "      <td>0.819252</td>\n",
       "      <td>0.828565</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.604525</td>\n",
       "      <td>0.058988</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>2.0</td>\n",
       "      <td>{'C': 2.0}</td>\n",
       "      <td>0.844562</td>\n",
       "      <td>0.818513</td>\n",
       "      <td>0.819252</td>\n",
       "      <td>0.827443</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       2.843833      0.618862         0.007690        0.002818     0.5   \n",
       "3       3.589835      0.049526         0.009075        0.005482     4.0   \n",
       "1       3.531672      0.106731         0.009840        0.000877     1.0   \n",
       "2       3.604525      0.058988         0.002560        0.002347     2.0   \n",
       "\n",
       "       params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.5}           0.842341           0.838974           0.826893   \n",
       "3  {'C': 4.0}           0.847521           0.818513           0.822212   \n",
       "1  {'C': 1.0}           0.843655           0.822787           0.819252   \n",
       "2  {'C': 2.0}           0.844562           0.818513           0.819252   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.836070        0.006633                1  \n",
       "3         0.829416        0.012891                2  \n",
       "1         0.828565        0.010768                3  \n",
       "2         0.827443        0.012109                4  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LinearSVC 超参数搜索\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(class_weight='balanced', dual=True)\n",
    "lsvc_params = {\n",
    "    'C': [0.1,0.5, 1.0, 2.0, 4.0]\n",
    "}\n",
    "lsvc_gs, lsvc_log = run_grid_search(lsvc, lsvc_params, trainX, trainY)\n",
    "\n",
    "lsvc_best = lsvc_gs.best_estimator_\n",
    "val_pred = lsvc_best.predict(valX)\n",
    "lsvc_val_bacc = balanced_accuracy_score(val_y, val_pred)\n",
    "print('LinearSVC best params:', lsvc_gs.best_params_, 'val balanced_acc:', lsvc_val_bacc)\n",
    "\n",
    "lsvc_log.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "SGD best params: {'alpha': 0.0001, 'loss': 'log_loss'} val balanced_acc: 0.878622948577077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288690</td>\n",
       "      <td>0.031615</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>{'alpha': 0.0001, 'loss': 'log_loss'}</td>\n",
       "      <td>0.846207</td>\n",
       "      <td>0.838159</td>\n",
       "      <td>0.822562</td>\n",
       "      <td>0.835643</td>\n",
       "      <td>0.009816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.301628</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.001</td>\n",
       "      <td>hinge</td>\n",
       "      <td>{'alpha': 0.001, 'loss': 'hinge'}</td>\n",
       "      <td>0.856160</td>\n",
       "      <td>0.809966</td>\n",
       "      <td>0.830759</td>\n",
       "      <td>0.832295</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.186328</td>\n",
       "      <td>0.074526</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.001</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>{'alpha': 0.001, 'loss': 'log_loss'}</td>\n",
       "      <td>0.823983</td>\n",
       "      <td>0.836621</td>\n",
       "      <td>0.816419</td>\n",
       "      <td>0.825674</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.286169</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>hinge</td>\n",
       "      <td>{'alpha': 0.0001, 'loss': 'hinge'}</td>\n",
       "      <td>0.835032</td>\n",
       "      <td>0.833406</td>\n",
       "      <td>0.787689</td>\n",
       "      <td>0.818709</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.255330</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.019929</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>{'alpha': 0.0005, 'loss': 'log_loss'}</td>\n",
       "      <td>0.849997</td>\n",
       "      <td>0.815162</td>\n",
       "      <td>0.769780</td>\n",
       "      <td>0.811646</td>\n",
       "      <td>0.032843</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "1       0.288690      0.031615         0.008637        0.005280      0.0001   \n",
       "4       0.301628      0.024322         0.005706        0.004068       0.001   \n",
       "5       0.186328      0.074526         0.002641        0.003735       0.001   \n",
       "0       0.286169      0.020317         0.006506        0.004788      0.0001   \n",
       "3       0.255330      0.012333         0.019929        0.009129      0.0005   \n",
       "\n",
       "  param_loss                                 params  split0_test_score  \\\n",
       "1   log_loss  {'alpha': 0.0001, 'loss': 'log_loss'}           0.846207   \n",
       "4      hinge      {'alpha': 0.001, 'loss': 'hinge'}           0.856160   \n",
       "5   log_loss   {'alpha': 0.001, 'loss': 'log_loss'}           0.823983   \n",
       "0      hinge     {'alpha': 0.0001, 'loss': 'hinge'}           0.835032   \n",
       "3   log_loss  {'alpha': 0.0005, 'loss': 'log_loss'}           0.849997   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "1           0.838159           0.822562         0.835643        0.009816   \n",
       "4           0.809966           0.830759         0.832295        0.018890   \n",
       "5           0.836621           0.816419         0.825674        0.008334   \n",
       "0           0.833406           0.787689         0.818709        0.021945   \n",
       "3           0.815162           0.769780         0.811646        0.032843   \n",
       "\n",
       "   rank_test_score  \n",
       "1                1  \n",
       "4                2  \n",
       "5                3  \n",
       "0                4  \n",
       "3                5  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGDClassifier (hinge/log) 超参数搜索\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(class_weight='balanced', random_state=100, max_iter=2000, tol=1e-3)\n",
    "sgd_params = {\n",
    "    'loss': ['hinge', 'log_loss'],\n",
    "    'alpha': [1e-4, 5e-4, 1e-3],\n",
    "}\n",
    "sgd_gs, sgd_log = run_grid_search(sgd, sgd_params, trainX, trainY)\n",
    "\n",
    "sgd_best = sgd_gs.best_estimator_\n",
    "val_pred = sgd_best.predict(valX)\n",
    "sgd_val_bacc = balanced_accuracy_score(val_y, val_pred)\n",
    "print('SGD best params:', sgd_gs.best_params_, 'val balanced_acc:', sgd_val_bacc)\n",
    "\n",
    "sgd_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN（Keras）的模型结构解释\n",
    "- 使用训练集拟合分词器，仅在验证/测试集做 transform，避免信息泄漏。\n",
    "- 模型结构：Embedding → Conv1D(k=3/4/5) → GlobalMaxPool → Concat → Dropout → Dense(3)。\n",
    "- 监控验证集，早停，指标使用 balanced accuracy 进行报告（训练优化目标仍为交叉熵）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14774\\AppData\\Local\\Temp\\ipykernel_23176\\1483049522.py:52: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_textcnn, epochs=15, batch_size=64, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14774\\.conda\\envs\\py38tflow2\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "47/47 [==============================] - 7s 125ms/step - loss: 0.6230 - accuracy: 0.8221\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 5s 102ms/step - loss: 0.3362 - accuracy: 0.8734\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 5s 102ms/step - loss: 0.1582 - accuracy: 0.9394\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 5s 104ms/step - loss: 0.1028 - accuracy: 0.9665\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.0680 - accuracy: 0.9816\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 5s 101ms/step - loss: 0.0451 - accuracy: 0.9876\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 5s 99ms/step - loss: 0.0300 - accuracy: 0.9910\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 5s 104ms/step - loss: 0.0241 - accuracy: 0.9913\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 5s 100ms/step - loss: 0.0173 - accuracy: 0.9943\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 5s 99ms/step - loss: 0.0150 - accuracy: 0.9936\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 5s 99ms/step - loss: 0.0128 - accuracy: 0.9963\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 5s 99ms/step - loss: 0.0109 - accuracy: 0.9956\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 5s 101ms/step - loss: 0.0116 - accuracy: 0.9950\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 5s 101ms/step - loss: 0.0087 - accuracy: 0.9966\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 5s 102ms/step - loss: 0.0084 - accuracy: 0.9963\n",
      "Best: 0.833640 using {'dropout': 0.4, 'emb_dim': 64, 'filter_sizes': (2, 3, 4), 'num_filters': 128}\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 5s 148ms/step - loss: 0.7326 - accuracy: 0.8084 - val_loss: 0.6323 - val_accuracy: 0.8031\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 3s 136ms/step - loss: 0.5247 - accuracy: 0.8221 - val_loss: 0.4883 - val_accuracy: 0.8031\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 3s 125ms/step - loss: 0.3451 - accuracy: 0.8650 - val_loss: 0.2612 - val_accuracy: 0.8975\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.1812 - accuracy: 0.9333 - val_loss: 0.1815 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 3s 131ms/step - loss: 0.1254 - accuracy: 0.9605 - val_loss: 0.1528 - val_accuracy: 0.9431\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 3s 134ms/step - loss: 0.0909 - accuracy: 0.9745 - val_loss: 0.1348 - val_accuracy: 0.9524\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 3s 138ms/step - loss: 0.0690 - accuracy: 0.9802 - val_loss: 0.1252 - val_accuracy: 0.9524\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 0.0510 - accuracy: 0.9886 - val_loss: 0.1175 - val_accuracy: 0.9571\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0398 - accuracy: 0.9899 - val_loss: 0.1164 - val_accuracy: 0.9551\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 0.0304 - accuracy: 0.9933 - val_loss: 0.1162 - val_accuracy: 0.9558\n",
      "TextCNN val balanced_acc: 0.8561361760903045\n",
      "TextCNN test balanced_acc: 0.8555784641476395\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # 用于文本分词，将文本转为词的索引序列\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # 用于将序列填充或截断到固定长度\n",
    "from tensorflow.keras import layers, models  # 导入网络层和模型相关模块\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier  # 用于将 Keras 模型包装成 scikit-learn 可用的分类器\n",
    "from sklearn.model_selection import GridSearchCV  # 网格搜索超参数\n",
    "from sklearn.metrics import balanced_accuracy_score  # 用于计算平衡准确率\n",
    "\n",
    "# 仅基于训练集拟合分词器：避免验证集/测试集信息泄露到分词器中\n",
    "max_words = 20000  # 设定词汇表最大大小，保留最常见的20000个词\n",
    "max_len = 150  # 设定序列的最大长度，超过则截断，不足则填充\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True, oov_token='<OOV>')  # 创建分词器，lower=True将文本转为小写，oov_token处理未见过的词\n",
    "tokenizer.fit_on_texts(traintxt_norm)  # 仅在训练集文本上拟合分词器\n",
    "\n",
    "# 序列化与填充：将文本转为数字序列，并统一长度\n",
    "X_train_seq = tokenizer.texts_to_sequences(traintxt_norm)  # 训练集文本转序列\n",
    "X_val_seq = tokenizer.texts_to_sequences(valtxt_norm)  # 验证集文本转序列\n",
    "X_test_seq = tokenizer.texts_to_sequences(testtxt_norm)  # 测试集文本转序列\n",
    "\n",
    "# 对序列进行填充或截断，使所有序列长度为max_len，padding和truncating设为'post'表示在序列末尾进行操作\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "num_classes = 3  # 分类任务的类别数\n",
    "\n",
    "# TextCNN 模型构建函数：构建文本卷积神经网络模型，参数由超参数搜索传入\n",
    "def build_textcnn(emb_dim=64, num_filters=64, filter_sizes=(3, 4, 5), dropout=0.5):\n",
    "    vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "    inputs = layers.Input(shape=(max_len,))  # 定义输入层，输入形状为(max_len,)\n",
    "    # 嵌入层：将词的索引转为稠密向量表示，input_dim为词汇表大小，output_dim为嵌入维度，input_length为输入序列长度\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_len)(inputs)\n",
    "    convs = []  # 用于存储不同卷积核处理后的特征\n",
    "    for k in filter_sizes:  # 遍历不同尺寸的卷积核\n",
    "        # 一维卷积层：num_filters为卷积核数量，k为卷积核尺寸，activation为激活函数，padding为'valid'表示不填充\n",
    "        c = layers.Conv1D(num_filters, k, activation='relu', padding='valid')(x)\n",
    "        # 全局最大池化层：对每个卷积结果取最大值，捕捉最显著的特征\n",
    "        p = layers.GlobalMaxPooling1D()(c)\n",
    "        convs.append(p)  # 将池化结果加入列表\n",
    "    # 拼接不同卷积核得到的特征，若只有一个卷积核则直接取该特征\n",
    "    x = layers.Concatenate()(convs) if len(convs) > 1 else convs[0]\n",
    "    x = layers.Dropout(dropout)(x)  # Dropout层：防止过拟合，随机丢弃部分神经元\n",
    "    # 输出层：num_classes个输出，softmax激活函数将输出转为概率分布\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)  # 构建模型，指定输入和输出\n",
    "    # 编译模型：optimizer为优化器，loss为损失函数，metrics为评估指标\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 将 Keras 模型包装为 scikit-learn 分类器\n",
    "model = KerasClassifier(build_fn=build_textcnn, epochs=15, batch_size=64, verbose=1)\n",
    "\n",
    "# 定义要搜索的超参数网格\n",
    "param_grid = {\n",
    "    'emb_dim': [64, 128],  # 嵌入维度候选值\n",
    "    'num_filters': [64, 128],  # 卷积核数量候选值\n",
    "    'dropout': [0.3, 0.4],  # Dropout 率候选值\n",
    "    'filter_sizes': [(3, 4, 5), (2, 3, 4)]  # 卷积核尺寸组合候选值\n",
    "}\n",
    "\n",
    "# 创建网格搜索对象\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='balanced_accuracy', verbose=2,n_jobs=-1)\n",
    "\n",
    "# 执行超参数搜索（使用训练集数据，这里为了演示，实际可根据需求调整）\n",
    "grid_result = grid.fit(X_train_pad, np.array(trainY))\n",
    "\n",
    "# 输出最佳超参数和最佳分数\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# 使用最佳超参数构建模型\n",
    "best_model = build_textcnn(\n",
    "    emb_dim=grid_result.best_params_['emb_dim'],\n",
    "    num_filters=grid_result.best_params_['num_filters'],\n",
    "    dropout=grid_result.best_params_['dropout'],\n",
    "    filter_sizes=grid_result.best_params_['filter_sizes']\n",
    ")\n",
    "\n",
    "# 回调：早停：当验证集损失在patience轮内没有改善时停止训练，并恢复最佳权重\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 训练最佳模型\n",
    "history = best_model.fit(\n",
    "    X_train_pad, np.array(trainY),\n",
    "    validation_data=(X_val_pad, np.array(val_y)),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 验证集性能（balanced accuracy）：计算平衡准确率，解决类别不平衡问题\n",
    "val_pred = best_model.predict(X_val_pad, batch_size=256, verbose=0).argmax(axis=1)  # 预测验证集并取概率最大的类别\n",
    "val_bacc = balanced_accuracy_score(val_y, val_pred)  # 计算平衡准确率\n",
    "print('TextCNN val balanced_acc:', val_bacc)  # 打印验证集平衡准确率\n",
    "\n",
    "# 测试集评估：在测试集上评估模型性能\n",
    "test_pred = best_model.predict(X_test_pad, batch_size=256, verbose=0).argmax(axis=1)  # 预测测试集并取概率最大的类别\n",
    "test_bacc = balanced_accuracy_score(test_true, test_pred)  # 计算测试集平衡准确率\n",
    "print('TextCNN test balanced_acc:', test_bacc)  # 打印测试集平衡准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选出最好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>val_bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.882869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>{'alpha': 0.0001, 'loss': 'log_loss'}</td>\n",
       "      <td>0.878623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 2.0}</td>\n",
       "      <td>0.876533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.803955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                            best_params  val_bal_acc\n",
       "2           LinearSVC                             {'C': 0.1}     0.882869\n",
       "3       SGDClassifier  {'alpha': 0.0001, 'loss': 'log_loss'}     0.878623\n",
       "1  LogisticRegression                             {'C': 2.0}     0.876533\n",
       "0       MultinomialNB                         {'alpha': 0.5}     0.803955"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于验证集 balanced accuracy 选择表现最佳的模型\n",
    "cv_summary = pd.DataFrame([\n",
    "    ['MultinomialNB', mnb_gs.best_params_, mnb_val_bacc],\n",
    "    ['LogisticRegression', lr_gs.best_params_, lr_val_bacc],\n",
    "    ['LinearSVC', lsvc_gs.best_params_, lsvc_val_bacc],\n",
    "    ['SGDClassifier', sgd_gs.best_params_, sgd_val_bacc],\n",
    "], columns=['model', 'best_params', 'val_bal_acc']).sort_values('val_bal_acc', ascending=False)\n",
    "cv_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去测试集验证去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test balanced_acc: 0.8740838209168876\n",
      "[[1188    2    1]\n",
      " [  14   97   18]\n",
      " [   2   20  151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal     0.9867    0.9975    0.9921      1191\n",
      "        spam     0.8151    0.7519    0.7823       129\n",
      "    smishing     0.8882    0.8728    0.8805       173\n",
      "\n",
      "    accuracy                         0.9618      1493\n",
      "   macro avg     0.8967    0.8741    0.8849      1493\n",
      "weighted avg     0.9605    0.9618    0.9610      1493\n",
      "\n",
      "\n",
      "示例-分类正确：\n",
      "[gold=normal, pred=normal] Hey i booked the kb on sat already... what other lessons are we going for ah? Keep your sat night free we need to meet and confirm our lodging\n",
      "[gold=normal, pred=normal] Hmm .. Bits and pieces lol ... *sighs* ...\n",
      "[gold=normal, pred=normal] 10 min later k...\n",
      "[gold=smishing, pred=smishing] Your B4U voucher w/c 27/03 is MARSMS. Log onto www.B4Utele.com for discount credit. To opt out reply stop. Customer care call 08717168528\n",
      "[gold=normal, pred=normal] I dont have that much image in class.\n",
      "[gold=normal, pred=normal] Want to finally have lunch today?\n",
      "[gold=smishing, pred=smishing] URGENT! Your Mobile number has been awarded with a £2000 prize GUARANTEED. Call 09058094455 from land line. Claim 3030. Valid 12hrs only\n",
      "[gold=normal, pred=normal] Y lei?\n",
      "[gold=normal, pred=normal] Yeah, we got one lined up for us\n",
      "[gold=normal, pred=normal] Hello which the site to download songs its urgent pls\n",
      "\n",
      "示例-分类错误与可能原因：\n",
      "[gold=smishing, pred=spam] FREE2DAY sexy St George's Day pic of Jordan!Txt PIC to 89080 dont miss out, PocketBabe.co.uk 0870241182716 \n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=spam, pred=normal] thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???\n",
      "  可能原因：含“中奖/免费”等强提示词\n",
      "[gold=smishing, pred=normal] Hey I am really * want to chat or see me  text to 08718738034\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=spam, pred=smishing] Can U get 2 phone NOW? I wanna chat 2 set up meet Call me NOW on 09096102316 U can cum here 2moro Luv JANE xx Calls£1/minmoremobsEMSPOBox45PO139WA\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=spam, pred=smishing] New Offer! Save upto 40\\% electricity bill with Power Saver(GOVT. LAB TESTED), Rs. 1050/-(free home delivery) 3 Yr. Guarantee Call 9891943823,9891943780\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=spam, pred=smishing] You are being ripped off! Get your mobile content from www.clubmoby.com call 08717509990 poly/true/Pix/Ringtones/Games six downloads for only 3\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=normal, pred=smishing] We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
      "  可能原因：含“中奖/免费”等强提示词\n",
      "[gold=smishing, pred=spam] \tThe current leading bid is 151. To pause this auction send OUT. Customer Care: 08718726270\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=smishing, pred=spam] \tI'd like to tell you my deepest darkest fantasies. Call me 09094646631 just 60p/min. To stop texts call 08712460324 (nat rate)\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n",
      "[gold=spam, pred=smishing] We tried to contact you re your reply to our offer of a Video Phone 750 anytime any network mins Half Price Line Rental Camcorder Reply or call 08000930705\n",
      "  可能原因：含URL/长号码，字符模式可能误导；含“中奖/免费”等强提示词\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上做最终预测，并展示正确/错误样本（复用已构建的 testX/test_true/test_indices）\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 选择验证集上最优的模型\n",
    "best_row = cv_summary.iloc[0]\n",
    "best_name = best_row['model']\n",
    "\n",
    "if best_name == 'MultinomialNB':\n",
    "    best_model = mnb_best\n",
    "    X_test_for_best = testX_tfidf\n",
    "elif best_name == 'LogisticRegression':\n",
    "    best_model = lr_best\n",
    "    X_test_for_best = testX\n",
    "elif best_name == 'LinearSVC':\n",
    "    best_model = lsvc_best\n",
    "    X_test_for_best = testX\n",
    "elif best_name == 'SGDClassifier':\n",
    "    best_model = sgd_best\n",
    "    X_test_for_best = testX\n",
    "else:\n",
    "    # fallback\n",
    "    best_model = lr_best\n",
    "    X_test_for_best = testX\n",
    "\n",
    "# 预测（根据模型选择相应的测试特征矩阵）\n",
    "pred_test = best_model.predict(X_test_for_best)\n",
    "\n",
    "# 评估与示例\n",
    "print('Test balanced_acc:', balanced_accuracy_score(test_true, pred_test))\n",
    "print(confusion_matrix(test_true, pred_test))\n",
    "print(classification_report(test_true, pred_test, target_names=classlabels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可能是分类不平衡导致的，尝试一下手动调整增加一下它们的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14774\\.conda\\envs\\py38tflow2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1.0, 'class_weight': {0: 1, 1: 3, 2: 3}}\n",
      "Best score: 0.8492480232496532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14774\\.conda\\envs\\py38tflow2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义包含不同权重组合的参数网格\n",
    "param_grid = {\n",
    "    'C':[0.1,1.0],\n",
    "    'class_weight':\n",
    "     [{0:1, 1:3, 2:3}]}       # 大幅度调高\n",
    "\n",
    "# 用 GridSearchCV 搜索最优参数（需要把 LinearSVC 包装进 GridSearchCV）\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    LinearSVC(dual=True),\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',  # 用平衡准确率评估（适合类不平衡）\n",
    "    cv=3  # 3折交叉验证\n",
    ")\n",
    "grid.fit(trainX, trainY)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 看来效果没有很明显的提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "- 特征处理方面，除了TF-IDF，可以再试试一些SMS特定特征工程。\n",
    "- 类别不均衡的实际影响：只有 `class_weight='balanced'` 还不够，对 spam和smishing的区分方便效果可以做到更好，比如可以试试重采样技术，补足这两类样本偏小的问题。\n",
    "- 模型选择方面，除了以上展示的这些,还可以试试更多复杂的集成模型（梯度提升、AdaBoost、投票分类器）和别的现代方法（XGBoost、LightGBM）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38tflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
